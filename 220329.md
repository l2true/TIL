# 220329 ğŸ•



## Tensorflow



### 1. ì‹ ê²½ë§ì˜ êµ¬ì¡°

- ì‹ ê²½ë§ í›ˆë ¨ì— ê´€ë ¨ëœ ìš”ì†Œë“¤
  - Layer(ì¸µ): ë„¤íŠ¸ì›Œí¬ë¥¼ êµ¬ì„±
  - Data(ì…ë ¥ ë°ì´í„°) & Label(íƒ€ê²Ÿ ë°ì´í„°)
  - **ì†ì‹¤í•¨ìˆ˜** **(Loss functino)**: í•™ìŠµì— ì‚¬ìš©í•  í”¼ë“œë°± ì‹ í˜¸ë¥¼ ì •ì˜
    - MSE(Regressionë¬¸ì œ)
    - CTC(ì‹œí€€ìŠ¤ í•™ìŠµ)
    - Binary cross-entropy(2ê°œì˜ class ë¶„ë¥˜)
    - Categorical corss-entropy(nê°œì˜ classë¶„ë¥˜)
  - **ì˜µí‹°ë§ˆì´ì €(Optimizer)**: í•™ìŠµ ì§„í–‰ë°©ì‹ì„ ê²°ì • -> ì†ì‹¤í•¨ìˆ˜ë¥¼ ì¤„ì´ëŠ” ê³¼ì •
    - GD(Full Batch Gradient Descent): ê²½ì‚¬ í•˜ê°•ë²•, Batch size = ì „ì²´
    - SGD(Stochastic Grdient Descent): Batch size = 1 -> í•œê°œì˜ í•™ìŠµ ë°ì´í„°ì”© ê°€ì¤‘ì¹˜ ì—…ë°ì´íŠ¸
    - Mini Batch Gradient Descent: Batch size = 2^nê°œ
      - SGDì˜ ë…¸ì´ì¦ˆë¥¼ ì¤„ì´ë©´ì„œë„ ì „ì²´ ë°°ì¹˜ë³´ë‹¤ëŠ” ë” ë¹ ë¥´ê²Œ ìµœì ì ì„ êµ¬í•¨

![image-20220329155301409](220329.assets/image-20220329155301409.png)

- Layerë€?
  - í•˜ë‚˜ ì´ìƒì˜ í…ì„œ(tensor, 2ì°¨ì› ì´ìƒì˜ í–‰ë ¬)ë¥¼ ì…ë ¥ìœ¼ë¡œ í•˜ë‚˜ ì´ìƒì˜ í…ì„œë¥¼ ì¶œë ¥í•˜ëŠ” ë°ì´í„° ì²˜ë¦¬ ëª¨ë“ˆ
  - ëŒ€ë¶€ë¶„ ê°€ì¤‘ì¹˜ë¼ëŠ” ìƒíƒœë¥¼ ê°€ì§
    - ê°€ì¤‘ì¹˜: í™•ë¥ ì  ê²½ì‚¬ í•˜ê°•ë²•ì— ì˜í•´ í•™ìŠµë˜ëŠ” í•˜ë‚˜ ì´ìƒì˜ í…ì„œ
    - ë„¤íŠ¸ì›Œí¬ê°€ í•™ìŠµí•œ ì •ë³´ê°€ ë‹´ê²¨ ìˆìŒ
  - ì¢…ë¥˜
    - **ì™„ì „ ì—°ê²°ì¸µ (Fully Connected)/ë°€ì§‘ì¸µ (Dense)**: ì…ë ¥ê³¼ ì¶œë ¥ì´ ëª¨ë‘ ì—°ê²°ëœ layer
      - dense ë ˆì´ì–´ë¡œë§Œ êµ¬ì„±ë˜ì–´ ìˆëŠ” ê²ƒì€ Neural Networkë¼ê³  ë¶€ë¦„
      - ì–´ë–¤ ëª¨ë¸ì´ë“  ë§ˆì§€ë§‰ ë…¸ë“œì—ì„œ ì£¼ë¡œ Denseë¥¼ ì‚¬ìš© 
      - 1ì°¨ì› ë°ì´í„°ë§Œ ë°›ì„ ìˆ˜ ìˆìŒ
    - **í•©ì„±ê³±ì¸µ (Convolution)**: ì…ë ¥ -> filter(kernel) -> ì¶œë ¥ (feature map)
      - CNN: filterë§Œ ì¡°ì •í•˜ë©´ ë˜ê¸° ë•Œë¬¸ì— íŒŒë¼ë¯¸í„° ê³„ìˆ˜ê°€ ì¤„ì–´ë“¦
      - 2ì°¨ì› ë°ì´í„°ë¥¼ ë°›ì„ ìˆ˜ ìˆìŒ: ì´ë¯¸ì§€ ë°ì´í„° 
    - **ìˆœí™˜ì¸µ (Recurrent)**: ì…ë ¥ -> hidden state -> ì¶œë ¥
      - RNN: íŒŒë¼ë¯¸í„°ëŠ” ëŠ˜ì–´ë‚˜ì§€ë§Œ, ì—°ì‚° ìƒì—ì„œ ìˆœì„œì— ëŒ€í•œ ê°œë…ì„ ë„ì…
      - ì‹œê³„ì—´ ë°ì´í„° 
      - 1ì°¨ì› ë°ì´í„°ë¥¼ ë°›ìŒ

- **Sequential vs Functional** 
  - ëª¨ë¸ êµ¬í˜„ ë°©ë²•
  - Sequential: layerì„ ìŒ“ì„ ë•Œ ì¼ìë¡œ í†µìœ¼ë¡œ ìŒ“ìŒ
  - Functional: layerë“¤ì„ ë§Œë“¤ì–´ ë†“ê³ , ì—°ê²°í•˜ëŠ” ê°œë… 
  - [functional API](https://wikidocs.net/38861)



### 2. Tensorflow ì„¤ì¹˜

- local ì—ì„¤ì¹˜ : GPU ë“œë¼ì´ë²„ ì„¤ì¹˜ -> CUDA -> CuDNN -> tensorflow gpu ì„¤ì¹˜ (ì¤‘ìš”!! version)
  - https://www.tensorflow.org/install/source_windows#tested_build_configurations
- NvidiaDocker + https://hub.docker.com/r/tensorflow/tensorflow/

- í´ë¼ìš°ë“œ ì´ìš©: Colab, GCP, AWS 

```python
import tensorflow as tf

tf.__version__ # tensorflow version í™•ì¸

tf.test.is_gpu_available() #GPU ì‚¬ìš©ê°€ëŠ¥ ì—¬ë¶€ ì²´í¬ # True

# ë‚œìˆ˜ ìƒì„±
import numpy as np
import matplotlib.pyplot as plt

np.random.seed(0) # seedê°’ ê³ ì •

def make_random_data():
    x = np.random.uniform(low=-2, high=2, size=200) # -2~2ê¹Œì§€ ì¤‘ 200ê°œ ë½‘ê¸°
    y = []
    for t in x:
        r = np.random.normal(loc=0.0, scale=(0.5 + t*t/3), size=None)
        y.append(r) # ì •ê·œë¶„í¬ì˜ ë…¸ì´ì¦ˆ ìƒì„±
    return  x, 2*x -1 + np.array(y) # ë…¸ì´ì¦ˆ ì¶”ê°€

# ëª¨ë¸ ìƒì„±
# keras ì—ì„œ model ì„ ìƒì„±í•˜ëŠ” ë°©ë²•
# ê³µí†µ: model ê°ì²´ ìƒì„± -> model compile -> model fit
# ë°©ë²•1. Sequential 
# ë°©ë²•2. Functional

#sequential ë°©ë²•ìœ¼ë¡œ model ìƒì„±
model = tf.keras.Sequential() # Sequential model ê°ì²´ ìƒì„±í›„ layer ì¶”ê°€ 
model.add(tf.keras.layers.Dense(units=1, input_dim=1)) 
#ì²«ë²ˆì§¸ layerì˜ input_dim : x.shape ì˜ ë§ˆì§€ë§‰ ì°¨ì›ê°’ (x.shape ì˜ ì²«ë²ˆì§¸ ì°¨ì›ì€ ë°ì´í„°ì˜ ê°¯ìˆ˜ )
#ë§ˆì§€ë§‰ layerì˜ units : y.shape ì˜ ë§ˆì§€ë§‰ ì°¨ì›ê°’ (ì‹¤ì œë¡œ ì˜ˆì¸¡í•  y_data ì˜ ì°¨ì›ì„ ì˜ë¯¸í•¨)

model.summary() #ìƒì„±ëœ model í™•ì¸ 

# ëª¨ë¸ í•™ìŠµ
model.compile(optimizer="sgd", loss="mse") 
# hyperparameter ë¥¼ ì§€ì •í•´ì¤€ë‹¤. 
# hpëŠ” listë¡œ ìƒì„±í›„ forë¬¸ì„ ì´ìš©í•´ í•™ìŠµì‹œí‚¬ìˆ˜ ìˆë‹¤. 

history = model.fit(x, y, epochs= 300)
# model ìƒì„±í›„ dataì— ë§ê²Œ weightë¥¼ fitting í•˜ëŠ” ê³¼ì •
# fitting ê²°ê³¼ëŠ” historyì— ì €ì¥í›„ ì¸¡ì •ê°’(loss) ë“±ì„ ë¶ˆëŸ¬ì˜¬ìˆ˜ ìˆë‹¤. 

# history ê·¸ë˜í”„ ê·¸ë¦¬ê¸° : epochsì— ë”°ë¥¸ lossì˜ ë³€í™”ë¥¼ ë³¼ ìˆ˜ ìˆë‹¤. 
epochs = np.arange(1, 300+1)
plt.plot(epochs, history.history["loss"])
plt.xlabel("epochs")
plt.ylabel("Loss")
plt.show()
```

![image-20220406101854516](220329.assets/image-20220406101854516.png)

```python
# í•™ìŠµ ì „ì˜ data
x_arr = np.arange(-2,2, 0.1)
y_arr = model.predict(x_arr)
plt.plot(x, y, "o")
plt.plot(x_arr, y_arr, "-r")
plt.show()
```

![image-20220406101914187](220329.assets/image-20220406101914187.png)

```python
# model ì´ dataì— fitting ëœ ìƒíƒœ 
x_arr = np.arange(-2,2, 0.1)
y_arr = model.predict(x_arr)
plt.plot(x, y, "o")
plt.plot(x_arr, y_arr, "-r")
plt.show()
```

![image-20220406101931879](220329.assets/image-20220406101931879.png)

```python
# í•¨ìˆ˜í˜• API
# input, output , layer ë¥¼ ë”°ë¡œ ìƒì„±í•´ ì¤˜ì•¼ í•œë‹¤. 
# ì´í›„ tf.keras.Model() ì´ë¼ëŠ” classë¥¼ í†µí•´ì„œ model ê°ì²´ë¥¼ ìƒì„±í•´ì¤€ë‹¤. 
input = tf.keras.Input(shape=(1,))
output = tf.keras.layers.Dense(1)(input) 
#ì•ë¶€ë¶„ì˜ layerë¥¼ ë’·ë¶€ë¶„ì˜ layerì— ì¸ìê°’ìœ¼ë¡œ ì…ë ¥í•œë‹¤. 
#í™•ì¥ì„± ìˆëŠ” modelì„ ìƒì„±í•  ìˆ˜ ìˆë‹¤. 

model_fun_api = tf.keras.Model(input, output)
model_fun_api.summary()

model_fun_api.compile(optimizer="sgd", loss="mse")

history_fun_api = model_fun_api.fit(x, y, epochs= 300)

# ëª¨ë¸ì„ í•™ìŠµì‹œí‚¨í›„ modelì„ loadí•˜ëŠ” ë°©ë²• -> ê°€ì¤‘ì¹˜ë§Œ ì¶”ê°€í•˜ì—¬ ë³„ë„ì˜ í•™ìŠµê³¼ì • ì—†ì´ ì‚¬ìš© 
model_fun_api.save("weights_sample.h5")

input = tf.keras.Input(shape=(1,))
output = tf.keras.layers.Dense(1)(input)

model_fun_api2 = tf.keras.Model(input, output)
model_fun_api2.summary()

model_fun_api2.load_weights("weights_sample.h5") # ê°€ì¤‘ì¹˜ ë¶ˆëŸ¬ì˜¤ê¸°

model_fun_api2.compile(optimizer="sgd", loss="mse")

# ì „ì´í•™ìŠµ : ì´ë¯¸ í•™ìŠµëœ modelì—ì„œ datasetì„ ì¶”ê°€í•´ ì¶”ê°€ í•™ìŠµì„ ì§„í–‰ì‹œí‚¨ë‹¤. modelì€ tensor hubì— ìˆë‹¤. 
```



### 3. Tensorboard

`<model_name>.callbacks.TensorBoard` ë¥¼ í†µí•´ í•™ìŠµë°ì´í„°ë¥¼ ì €ì¥í•œí›„ tensorboardë¥¼ ì´ìš©í•´ ì‹œê°í™” í• ìˆ˜ ìˆë‹¤. 

```python
# model ìƒì„±
callback_list = [tf.keras.callbacks.TensorBoard(log_dir="logs")]
model2.compile(optimizer="sgd", loss="mse")
history = model2.fit(x, y, epochs=30, callbacks=callback_list)

# model ì‹œê°í™”
logdir = "./logs"
tensorboard_callback = tf.keras.callbacks.TensorBoard(logdir)
%load_ext tensorboard
%tensorboard --logdir=./logs
```

![image-20220406103751430](220329.assets/image-20220406103751430.png)

ê·¸ ì™¸ ìì£¼ ì‚¬ìš©í•˜ëŠ”íˆ´

- ìƒìš© : https://wandb.ai/site : ì‹œê°í™”íˆ´

- opensource : katib : https://www.kubeflow.org/docs/components/katib/overview/

- tensorboard what if tool : https://www.tensorflow.org/tensorboard/what_if_tool



## Dense Layer

### 1. ì´ì§„ë¶„ë¥˜ (AND, OR, XOR) í•™ìŠµ

#### 1) AND

````python
import numpy as np
import tensorflow as tf

# AND
x_data = [[0,0],
          [0,1],
          [1,0],
          [1,1]]
y_data = [[0],
          [0],
          [0],
          [1]]

# ì—°ì‚°ì„ ìœ„í•´ listë¥¼ arrayí˜•íƒœë¡œ ë³€í™˜
# list ì˜ ìš©ë„ : ê°ì²´ ì €ì¥ì†Œ( íŒŒì´ì¬ì€ ëª¨ë“ ê²Œ ë‹¤ ê°ì²´ë‹¤. -> ëª¨ë“ ê±¸ ë‹¤ ë¦¬ìŠ¤íŠ¸ì— ë„£ì„ìˆ˜ ìˆë‹¤. )
# array : ìˆ«ì ì €ì¥ì†Œ, ìˆ«ìì˜ ì—°ì‚°ì´ ê°€ëŠ¥ (ì°¨ì›ì— ë”°ë¼ì„œ ìŠ¤ì¹¼ë¼, ë²¡í„°, í–‰ë ¬, í…ì„œ )
x_data = np.array(x_data) 
y_data = np.array(y_data)

# ë°ì´í„° í™•ì¸
x_data.shape # (4, 2) -> ë ˆì½”ë“œê°€ 4ê°œ, 2ì°¨ì› (í•˜ë‚˜ì˜ ë ˆì½”ë“œê°€ ê°’ì´ 2ê°œì”© êµ¬ì„±ë˜ì–´ ìˆìœ¼ë¯€ë¡œ)

# 1. sequential ê°ì²´ ìƒì„±
model_and = tf.keras.Sequential() 

# 2. Dense layer ì¶”ê°€
model_and.add(tf.keras.layers.Dense(1, activation="sigmoid", input_shape=(2,)))
# ë…¸ë“œ ê°¯ìˆ˜ = 1, í™œì„±í™” í•¨ìˆ˜ = sigmoid, ë°›ëŠ” ì°¨ì› = (2,)


# 3. compile 
model_and.compile(optimizer="sgd", loss="binary_crossentropy", metrics=["accuracy"])
# optimizer: ìµœì í™” ë°©ë²•(ë””í´íŠ¸ëŠ” adam), 
# loss: ì†ì‹¤í•¨ìˆ˜(ì´ì§„ë¶„ë¥˜ crossentropy) -> ì¶œë ¥ì´ 0ë˜ëŠ” 1
# mereics: ì„±ëŠ¥ ê¸°ì¤€ì˜ ê°„ì ‘ì ì¸ í‰ê°€ ì§€í‘œ. í•˜ì§€ë§Œ, í•™ìŠµë°ì´í„°ì™€ ì‹¤ì œ ì„±ëŠ¥ì˜ accuracyê°€ ë‹¬ë¼ì§€ëŠ” ì¼ë„ ë¹ˆë²ˆí•¨.

# 4. model fit 
model_and.fit(x_data, y_data, epochs=3000, batch_size=4) 
# epochs: ì „ì²´ ë°ì´í„°ì…‹ì„ 3000ë²ˆ ë³´ê² ë‹¤
# batch: ì „ì²´ ë°ì´í„°ë¥¼ í•œë²ˆì— 4ê°œì”© ë³´ê² ë‹¤. ê°€ì¤‘ì¹˜ê°€ ì—…ë°ì´íŠ¸ë˜ëŠ” ê¸°ì¤€

# 5. predict
# ì›ë˜ëŠ” testë°ì´í„°ë¡œ ë„£ì–´ì„œ ì§„í–‰. ì—¬ê¸°ì„œëŠ” ê·¸ëƒ¥ í•™ìŠµë°ì´í„°ë¡œ í•œ ê²ƒì„
model_and.predict(x_data) # array([[0.08041815],
                                # [0.2734547 ],
                                # [0.25232592],
                                # [0.5922486 ]], dtype=float32)

# ê°€ì¥ í° ê°’ì„ ì°¾ê¸°
np.round(model_and.predict(x_data)) # array([[0.],
                                           # [0.],
                                           # [0.],
                                           # [1.]], dtype=float32)	

````

```python
# í•™ìŠµëœ ê°€ì¤‘ì¹˜ë¡œ ê·¸ë˜í”„ ê·¸ë ¤ë³´ê¸°
import matplotlib.pyplot as plt

# ê°€ì¤‘ì¹˜ ë°›ì•„ì˜¤ê¸°
weights = model_and.layers[0].get_weights()[0]
biases = model_and.layers[0].get_weights()[1]

# ê·¸ë˜í”„ ê·¸ë¦¬ê¸°
plt.scatter(x_data[:,0], x_data[:,1], c=y_data)
x = np.linspace(np.amin(x_data[:,:1]),np.amax(x_data[:,:1]))

a = -weights[0]/weights[1]
b = -biases[0]/weights[1] 


plt.rcParams["figure.figsize"] = (15,5)
plt.plot(x, [a*i + b for i in x], color='green') # ì„ ë¶„ì„ ê¸°ì¤€ìœ¼ë¡œ ê°’ì˜ ì˜ì—­ì„ ë‚˜ëˆŒ ìˆ˜ ìˆìŒ

plt.show()
```

![image-20220401163559380](220329.assets/image-20220401163559380.png)

```python
# running_rate ì¡°ì ˆí•˜ëŠ” ë²• (ë””í´íŠ¸ëŠ” 0.01)
sgd = tf.keras.optimizers.SGD(learning_rate = 0.001) 
model.compile(optimizer = sgd, loss = "binary_crossentropy", metrics = ["accuracy"])
```



#### 2) OR

```PYTHON
import numpy as np
import tensorflow as tf
import matplotlib.pyplot as plt

x_data = [[0,0],
          [0,1],
          [1,0],
          [1,1]]
y_data = [[0],
          [1],
          [1],
          [1]]

x_data = np.array(x_data)
y_data = np.array(y_data)

# ëª¨ë¸ ìƒì„±
model_or = tf.keras.Sequential()
model_or.add(tf.keras.layers.Dense(1, activation="sigmoid", input_shape=(2,)))
model_or.compile(optimizer="sgd", loss="binary_crossentropy", metrics=["accuracy"])

# ëª¨ë¸ í•™ìŠµ
model_or.fit(x_data, y_data, epochs=3000,batch_size=4)

# ì˜ˆì¸¡
np.round(model_or.predict(x_data)) # array([[0.],
                                          # [1.],
                                          # [1.],
                                          # [1.]], dtype=float32)

# ê°€ì¤‘ì¹˜ ê°€ì ¸ì˜¤ê¸°
model_or.layers[0].get_weights() # [array([[2.3614073],
                                 # [2.0397925]], dtype=float32), array([-0.38158], dtype=float32)]

# ê°€ì¤‘ì¹˜ ê·¸ë˜í”„ ê·¸ë¦¬ê¸°
weights = model_or.layers[0].get_weights()[0] 
biases = model_or.layers[0].get_weights()[1]
plt.scatter(x_data[:,0], x_data[:,1], c=y_data)
x = np.linspace(np.amin(x_data[:,:1]),np.amax(x_data[:,:1]))

a = -weights[0]/weights[1]
b = -biases[0]/weights[1] 


plt.rcParams["figure.figsize"] = (15,5)
plt.plot(x, [a*i + b for i in x], color='green')

plt.show()
```

![image-20220401163858236](220329.assets/image-20220401163858236.png)





#### 3) XOR

- XORì€ ë…¸ë“œ 1ê°œ ë§Œìœ¼ë¡œëŠ” ì„ í˜• ê·¸ë˜í”„ë¥¼ ê·¸ë¦´ ìˆ˜ ì—†ìŒ. ë”°ë¼ì„œ ë…¸ë“œ 2ê°œë¥¼ ì‚¬ìš© -> Layer 2ê°œ

```PYTHON
import numpy as np
import tensorflow as tf
import matplotlib.pyplot as plt

x_data = [[0,0],
          [0,1],
          [1,0],
          [1,1]]
y_data = [[0],
          [1],
          [1],
          [0]]

x_data = np.array(x_data)
y_data = np.array(y_data)

x_data.shape # (4,2)
y_data.shape # (4,1)

# ëª¨ë¸ ìƒì„±
model_xor = tf.keras.Sequential()

# ë…¸ë“œ 2ê°œ ìƒì„±
model_xor.add(tf.keras.layers.Dense(2, activation = "sigmoid" ,input_shape = (2,)))
model_xor.add(tf.keras.layers.Dense(1, activation = "sigmoid"))

model_xor.compile(optimizer = tf.keras.optimizers.SGD(learning_rate=0.1), loss="binary_crossentropy", metrics=["accuracy"])

# í•™ìŠµ
model_xor.fit(x_data, y_data, batch_size=4, epochs=2000)

# ì˜ˆì¸¡
np.round(model_xor.predict(x_data)) # array([[0.],
                                           # [1.],
                                           # [1.],
                                           # [0.]], dtype=float32)

# ê°€ì¤‘ì¹˜ ê°€ì ¸ì˜¤ê¸°: ë…¸ë“œê°€ 2ê°œì´ë¯€ë¡œ ê°€ì¤‘ì¹˜ë„ 2ê°œì”©
weights_0 = model_xor.layers[0].get_weights()[0]
biases_0 = model_xor.layers[0].get_weights()[1]

weights_1 = model_xor.layers[1].get_weights()[0]
biases_1 = model_xor.layers[1].get_weights()[1]

# ê°€ì¤‘ì¹˜ë¡œ ê·¸ë˜í”„ ê·¸ë¦¬ê¸°
plt.scatter(x_data[:,0], x_data[:,1], c=y_data)
x = np.linspace(np.amin(x_data[:,:1]), np.amax(x_data[:,:1]))

a_0 = -weights_0[0]/weights_0[1]
b_0 = -biases_0[0]/weights_0[1] 

a_1 = -weights_1[0]/weights_1[1]
b_1 = -biases_1[0]/weights_1[1] 


plt.rcParams["figure.figsize"] = (15,5)
plt.plot(x, [a_0*i + b_0 for i in x], color='green')
plt.plot(x, [a_1*i + b_1 for i in x], color='red')


plt.show()
```

![image-20220401171526795](220329.assets/image-20220401171526795.png)



### 2. ì´ì§„ë¶„ë¥˜ ì‹¤ìŠµ

- dataë¥¼ x_dataì™€ y_dataë¡œ ë¶„ë¥˜í•˜ê¸°

```python
# data íƒìƒ‰
import numpy as np

data = [
    [0.72,0.82,-1],
    [0.91,-0.69,-1],
    [0.03,0.93,-1],
    [0.12,0.25,-1],
    [0.96,0.47,-1],
    [0.8,-0.75,-1],
    [0.46,0.98,-1],
    [0.66,0.24,-1],
    [0.72,-0.15,-1],
    [0.35,0.01,-1],
    [-0.11,0.1,1],
    [0.31,-0.96,1],
    [0.0,-0.26,1],
    [-0.43,-0.65,1],
    [0.57,-0.97,1],
    [-0.72,-0.64,1],
    [-0.25,-0.43,1],
    [-0.12,-0.9,1],
    [-0.58,0.62,1],
    [-0.77,-0.76,1]
]

data = np.array(data) # list to numpy
data.shape  # (20, 3)

x_data = data[:, 0:2]
y_data = data[:, 2:]

x_data.shape # input_shapeì„ ê²°ì •í•˜ê¸° ìœ„í•´ x_dataì˜ ì°¨ì› í™•ì¸ # (20, 2)
y_data.shape # out_layer ì˜ units ê°¯ìˆ˜ë¥¼ ê²°ì •í•˜ê¸° ìœ„í•´ y_data ì˜ ì°¨ì› í™•ì¸ 
```

- **Data scaling**
  - -1 ~ +1 ê¹Œì§€ì˜ ë°ì´í„°ë¥¼ 0~1 ê¹Œì§€ì˜ ê°’ìœ¼ë¡œ ë³€ê²½í•´ì¤€ë‹¤. 
  - activation function ë˜ëŠ” loss function ì˜ ê³„ì‚° ê²°ê³¼ê°€ ì•ˆì •ì ìœ¼ë¡œ ë‚˜ì˜¤ê¸° ìœ„í•´ 
  - í•™ìŠµë„ ë” ì˜ëœë‹¤. ANN ë¿ë§Œ ì•„ë‹ˆë¼. RNN, CNN ë“±ì—ë„S ì‚¬ìš©í•œë‹¤. 
  - ë°©ë²•1. ìˆ˜ì‹ìœ¼ë¡œ ë³€ê²½í•´ì¤€ë‹¤. y_data = (y_data+1)/2
  - ë°©ë²•2. module ì„ ì‚¬ìš©í•œë‹¤. (sklearn)

```python
from sklearn.preprocessing import MinMaxScaler

scaler = MinMaxScaler() #ê°ì²´ ìƒì„±
scaler.fit(y_data) #dataì— ë§ê²Œ ê°ì²´ ë³€ê²½
y_data = scaler.transform(y_data) # ë³€ê²½ëœ ê°ì²´ë¥¼ í†µí•´ data ë³€ê²½

y_data # array([[0.],
              # [0.],
              # [0.],
scaler.inverse_transform(y_data) # array([[-1.],
       									# [-1.],
       									# [-1.],
```

- ê·¸ë˜í”„ë¥¼ ê·¸ë¦¬ê³ , ì´ ì ë“¤ì„ ë‚˜ëˆ„ëŠ” ëª¨ë¸ ìƒì„± (ì§ì„  ê¸‹ê¸°)

```python
# data ê·¸ë˜í”„
import matplotlib.pyplot as plt
plt.scatter(x_data[:,0],x_data[:,1], c=y_data)

import tensorflow as tf

model = tf.keras.Sequential()
model.add(tf.keras.layers.Dense(1, activation="sigmoid", input_shape=(2,)))

model.compile(optimizer="sgd", loss = "binary_crossentropy", metrics = ["accuracy"])

history = model.fit(x_data, y_data, batch_size=20, epochs = 1500)

# ì§ì„ ì„ ì¶”ê°€í•œ ê·¸ë˜í”„
weights = model.layers[0].get_weights()[0]
biases = model.layers[0].get_weights()[1]
plt.scatter(x_data[:,0], x_data[:,1], c=y_data)
x = np.linspace(np.amin(x_data[:,:1]),np.amax(x_data[:,:1]))

a = -weights[0]/weights[1]
b = -biases[0]/weights[1] 


plt.rcParams["figure.figsize"] = (15,5)
plt.plot(x, [a*i + b for i in x], color='green')

plt.show()

# ì˜ˆì¸¡
scaler.inverse_transform(np.round(model.predict(x_data))) # array([[0.],
                                                                # [0.],
                                                                # [0.],
scaler.inverse_transform(y_data) # array([[-1.],
                                        # [-1.],
                                        # [-1.],
```

![image-20220406115044934](220329.assets/image-20220406115044934.png)

![image-20220406115057447](220329.assets/image-20220406115057447.png)
