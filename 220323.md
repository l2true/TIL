# 220323 ğŸ•



## ì˜¤ì°¨ ì—­ì „íŒŒë²•(backpropagation)

- ì˜¤ì°¨ë¥¼ ì—­ë°©í–¥ìœ¼ë¡œ ì „íŒŒí•˜ëŠ” ë°©ë²•

- ì—°ì‡„ë²•ì¹™ì— ì˜í•œ 'êµ­ì†Œì  ë¯¸ë¶„'ì„ ë…¸ë“œ ê°ê°ì— ì „ë‹¬

  - êµ­ì†Œì  ê³„ì‚°: ìì‹ ê³¼ ì§ì ‘ ê´€ê³„ëœ ì‘ì€ ë²”ìœ„ë§Œì„ ê³„ì‚°í•˜ì—¬ ìì‹ ê³¼ ê´€ê³„ëœ ì •ë³´ë§Œìœ¼ë¡œ ê²°ê³¼ë¥¼ ì¶œë ¥
  - í•©ì„±í•¨ìˆ˜ì˜ ë¯¸ë¶„: í•©ì„±í•¨ìˆ˜ë¥¼ êµ¬ì„±í•˜ëŠ” ê° í•¨ìˆ˜ì˜ ë¯¸ë¶„ì˜ ê³±ìœ¼ë¡œ í‘œí˜„ ê°€ëŠ¥

  ![image](https://user-images.githubusercontent.com/100326309/160083780-8dc50c71-9cbb-448c-ae83-379cec84b4c7.png)


![image](https://user-images.githubusercontent.com/100326309/160083814-f1e5757a-637f-4f06-bf1b-3526ab01387c.png)

### 1. í™œì„±í™” í•¨ìˆ˜ ê³„ì¸µ êµ¬í˜„

#### 1) ReLU 

- ReLU í•¨ìˆ˜

![image](https://user-images.githubusercontent.com/100326309/160083862-54f83ef4-1d90-4c3f-9a7d-e12e4701ddb2.png)

- ReLU í•¨ìˆ˜ì˜ ë¯¸ë¶„

![image](https://user-images.githubusercontent.com/100326309/160083906-3e2fb86d-7a14-4c09-b584-2c08a12693b4.png)

- ì—­ì „íŒŒ ì•Œê³ ë¦¬ì¦˜
  - x > 0, xì˜ ë¯¸ë¶„(1) * yì˜ ë¯¸ë¶„ = yì˜ ë¯¸ë¶„ 
  - x <= 0, xì˜ ë¯¸ë¶„(0) * yì˜ ë¯¸ë¶„ = 0

![image](https://user-images.githubusercontent.com/100326309/160083930-8ea292bd-5bf8-4724-bbdd-8a7987f8f598.png)

- êµ¬í˜„

```python
import numpy as np
# T/Fë¡œ êµ¬ì„±ëœ array(mask) ë§Œë“¤ê¸°
x = np.array([[1.0,-0.5],[-2.0,-0.3]])
mask = (x <= 0)  # [[False  True]
                 # [ True  True]]

# Relu êµ¬í˜„
class Relu:
    def __init__(self):
        self.mask=None 
        
    def forward(self, x): # ìˆœì „íŒŒ
        self.mask=(x<=0)
        out = x.copy()
        out[self.mask] = 0
        return out
    
    def backward(self, dout): # ì—­ì „íŒŒ 
        dout[self.mask] = 0
        dx = dout
        return dx
```



#### 2) Sigmoid 

- SIgmoid í•¨ìˆ˜

![image](https://user-images.githubusercontent.com/100326309/160083951-544fa1f4-a143-41e2-808f-ab3a84d8ad6d.png)

- Sigmoid í•¨ìˆ˜ì˜ ë¯¸ë¶„

  ![image](https://user-images.githubusercontent.com/100326309/160083977-2582def7-e3cc-4c85-a0ba-18c574326119.png)

- ìˆœì „íŒŒ ì•Œê³ ë¦¬ì¦˜

  - exp ë…¸ë“œ: y = exp(x)	

  - / ë…¸ë“œ: y = 1/x

![image](https://user-images.githubusercontent.com/100326309/160084011-73642ee6-8f8c-463f-abbf-5b56f46854ee.png)

- ì—­ì „íŒŒ ì•Œê³ ë¦¬ì¦˜

  - 1ë‹¨ê³„: / ë…¸ë“œ -> y = 1/x

  - 2ë‹¨ê³„: + ë…¸ë“œ -> ê·¸ëŒ€ë¡œ ì „ë‹¬
  - 3ë‹¨ê³„: expë…¸ë“œ -> y = exp(x)
  - 4ë‹¨ê³„: **x ë…¸ë“œ** -> ìˆœì „íŒŒ ë•Œì˜ ê°’ì„ ì„œë¡œ ë°”ê¿” ê³±í•¨

  ![image](https://user-images.githubusercontent.com/100326309/160084049-818c9f55-d0c0-485b-9848-bfa661bfc735.png)

  - ì´ë¥¼ ê°„ë‹¨íˆ í‘œí˜„í•˜ë©´, ë‹¤ìŒê³¼ ê°™ì´ ì •ë¦¬í•  ìˆ˜ ìˆìŒ

  ![image](https://user-images.githubusercontent.com/100326309/160084087-9a2d457e-c679-48d8-aa44-1a43979b35cc.png)

- êµ¬í˜„

```python
class Sigmoid:
    def __init__(self):
        self.out = None
    
    def forward(self, x):
        out = 1/(1+np.exp(-x))
        self.out = out
        
        return out
    
    def backward(self, dout):
        dx = dout * (1.0 - self.out)*self.out
        return dx
```



### 2. ì˜¤ì°¨ì—­ì „íŒŒë²•ì„ ì´ìš©í•œ ì‹ ê²½ë§ êµ¬í˜„

```python
import sys, os
sys.path.append("./code/")
sys.path.append("./code/ch05/")

import numpy as np
from dataset.mnist import load_mnist # mnist ë°ì´í„° ê´€ë ¨ í•¨ìˆ˜
from two_layer_net import TwoLayerNet # ê¸°ìš¸ê¸° êµ¬í•˜ëŠ” í•¨ìˆ˜

# ë°ì´í„° ì½ê¸°
(x_train, t_train), (x_test, t_test) = load_mnist(normalize=True, one_hot_label=True)

network = TwoLayerNet(input_size=784, hidden_size=50, output_size=10)

# ë¯¸ë‹ˆë°°ì¹˜ ì²˜ë¦¬
iters_num = 10000
train_size = x_train.shape[0]
batch_size = 100
learning_rate = 0.1

# ê²°ê³¼ê°’ì„ ë‹´ì„ ë¦¬ìŠ¤íŠ¸ ìƒì„± (ê³„ì† ê°±ì‹ ë¨) 
train_loss_list = []
train_acc_list = []
test_acc_list = []

# ì—í­ ì„¤ì •
iter_per_epoch = max(train_size / batch_size, 1)

# ê¸°ìš¸ê¸° ê³„ì‚° ë° ê°±ì‹ 
for i in range(iters_num):
    batch_mask = np.random.choice(train_size, batch_size)
    x_batch = x_train[batch_mask]
    t_batch = t_train[batch_mask]
    
    # ê¸°ìš¸ê¸° ê³„ì‚°
    #grad = network.numerical_gradient(x_batch, t_batch) # ìˆ˜ì¹˜ ë¯¸ë¶„ ë°©ì‹
    grad = network.gradient(x_batch, t_batch) # ì˜¤ì°¨ì—­ì „íŒŒë²• ë°©ì‹(í›¨ì”¬ ë¹ ë¥´ë‹¤)
    
    # ê°±ì‹ 
    for key in ('W1', 'b1', 'W2', 'b2'):
        network.params[key] -= learning_rate * grad[key]
    
    loss = network.loss(x_batch, t_batch)
    train_loss_list.append(loss)
    
    if i % iter_per_epoch == 0:
        train_acc = network.accuracy(x_train, t_train)
        test_acc = network.accuracy(x_test, t_test)
        train_acc_list.append(train_acc)
        test_acc_list.append(test_acc)
        print("train_acc:{}".format(train_acc), "test_acc:{}".format(test_acc))
        # train_acc:0.09818333333333333 test_acc:0.0972
        # train_acc:0.90195 test_acc:0.9038 
        # ...
        # train_acc:0.9774 test_acc:0.9687
        # train_acc:0.9784333333333334 test_acc:0.9706
```



### 3. ìˆ˜ì¹˜ë¯¸ë¶„ì„ í†µí•´ ì˜¤ì°¨ì—­ì „íŒŒë²•ì„ í†µí•´ êµ¬í•œ ê¸°ìš¸ê¸° ê²€ì¦

```python
import sys, os
sys.path.append("./code/")
sys.path.append("./code/ch05/")

import numpy as np
from dataset.mnist import load_mnist # mnist ë°ì´í„° ê´€ë ¨ í•¨ìˆ˜
from two_layer_net import TwoLayerNet # ê¸°ìš¸ê¸° êµ¬í•˜ëŠ” í•¨ìˆ˜

# ë°ì´í„° ì½ê¸°
(x_train, t_train), (x_test, t_test) = load_mnist(normalize=True, one_hot_label=True)

network = TwoLayerNet(input_size=784, hidden_size=50, output_size=10)

x_batch = x_train[:3]
t_batch = t_train[:3]

grad_numerical = network.numerical_gradient(x_batch, t_batch)
grad_backprop = network.gradient(x_batch, t_batch)

# ê° ê°€ì¤‘ì¹˜ì˜ ì ˆëŒ€ ì˜¤ì°¨ì˜ í‰ê· ì„ êµ¬í•œë‹¤.
for key in grad_numerical.keys():
    diff = np.average( np.abs(grad_backprop[key] - grad_numerical[key]) )
    print(key + ":" + str(diff))
```



---

ê·¸ë¦¼ ì¶œì²˜: ë°‘ë°”ë‹¥ë¶€í„° ì‹œì‘í•˜ëŠ” ë”¥ëŸ¬ë‹
