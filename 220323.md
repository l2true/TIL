# 220323 🍕



## 오차 역전파법(backpropagation)

- 오차를 역방향으로 전파하는 방법

- 연쇄법칙에 의한 '국소적 미분'을 노드 각각에 전달

  - 국소적 계산: 자신과 직접 관계된 작은 범위만을 계산하여 자신과 관계된 정보만으로 결과를 출력
  - 합성함수의 미분: 합성함수를 구성하는 각 함수의 미분의 곱으로 표현 가능

  ![image](https://user-images.githubusercontent.com/100326309/160083780-8dc50c71-9cbb-448c-ae83-379cec84b4c7.png)


![image](https://user-images.githubusercontent.com/100326309/160083814-f1e5757a-637f-4f06-bf1b-3526ab01387c.png)

### 1. 활성화 함수 계층 구현

#### 1) ReLU 

- ReLU 함수

![image](https://user-images.githubusercontent.com/100326309/160083862-54f83ef4-1d90-4c3f-9a7d-e12e4701ddb2.png)

- ReLU 함수의 미분

![image](https://user-images.githubusercontent.com/100326309/160083906-3e2fb86d-7a14-4c09-b584-2c08a12693b4.png)

- 역전파 알고리즘
  - x > 0, x의 미분(1) * y의 미분 = y의 미분 
  - x <= 0, x의 미분(0) * y의 미분 = 0

![image](https://user-images.githubusercontent.com/100326309/160083930-8ea292bd-5bf8-4724-bbdd-8a7987f8f598.png)

- 구현

```python
import numpy as np
# T/F로 구성된 array(mask) 만들기
x = np.array([[1.0,-0.5],[-2.0,-0.3]])
mask = (x <= 0)  # [[False  True]
                 # [ True  True]]

# Relu 구현
class Relu:
    def __init__(self):
        self.mask=None 
        
    def forward(self, x): # 순전파
        self.mask=(x<=0)
        out = x.copy()
        out[self.mask] = 0
        return out
    
    def backward(self, dout): # 역전파 
        dout[self.mask] = 0
        dx = dout
        return dx
```



#### 2) Sigmoid 

- SIgmoid 함수

![image](https://user-images.githubusercontent.com/100326309/160083951-544fa1f4-a143-41e2-808f-ab3a84d8ad6d.png)

- Sigmoid 함수의 미분

  ![image](https://user-images.githubusercontent.com/100326309/160083977-2582def7-e3cc-4c85-a0ba-18c574326119.png)

- 순전파 알고리즘

  - exp 노드: y = exp(x)	

  - / 노드: y = 1/x

![image](https://user-images.githubusercontent.com/100326309/160084011-73642ee6-8f8c-463f-abbf-5b56f46854ee.png)

- 역전파 알고리즘

  - 1단계: / 노드 -> y = 1/x

  - 2단계: + 노드 -> 그대로 전달
  - 3단계: exp노드 -> y = exp(x)
  - 4단계: **x 노드** -> 순전파 때의 값을 서로 바꿔 곱함

  ![image](https://user-images.githubusercontent.com/100326309/160084049-818c9f55-d0c0-485b-9848-bfa661bfc735.png)

  - 이를 간단히 표현하면, 다음과 같이 정리할 수 있음

  ![image](https://user-images.githubusercontent.com/100326309/160084087-9a2d457e-c679-48d8-aa44-1a43979b35cc.png)

- 구현

```python
class Sigmoid:
    def __init__(self):
        self.out = None
    
    def forward(self, x):
        out = 1/(1+np.exp(-x))
        self.out = out
        
        return out
    
    def backward(self, dout):
        dx = dout * (1.0 - self.out)*self.out
        return dx
```



### 2. 오차역전파법을 이용한 신경망 구현

```python
import sys, os
sys.path.append("./code/")
sys.path.append("./code/ch05/")

import numpy as np
from dataset.mnist import load_mnist # mnist 데이터 관련 함수
from two_layer_net import TwoLayerNet # 기울기 구하는 함수

# 데이터 읽기
(x_train, t_train), (x_test, t_test) = load_mnist(normalize=True, one_hot_label=True)

network = TwoLayerNet(input_size=784, hidden_size=50, output_size=10)

# 미니배치 처리
iters_num = 10000
train_size = x_train.shape[0]
batch_size = 100
learning_rate = 0.1

# 결과값을 담을 리스트 생성 (계속 갱신됨) 
train_loss_list = []
train_acc_list = []
test_acc_list = []

# 에폭 설정
iter_per_epoch = max(train_size / batch_size, 1)

# 기울기 계산 및 갱신
for i in range(iters_num):
    batch_mask = np.random.choice(train_size, batch_size)
    x_batch = x_train[batch_mask]
    t_batch = t_train[batch_mask]
    
    # 기울기 계산
    #grad = network.numerical_gradient(x_batch, t_batch) # 수치 미분 방식
    grad = network.gradient(x_batch, t_batch) # 오차역전파법 방식(훨씬 빠르다)
    
    # 갱신
    for key in ('W1', 'b1', 'W2', 'b2'):
        network.params[key] -= learning_rate * grad[key]
    
    loss = network.loss(x_batch, t_batch)
    train_loss_list.append(loss)
    
    if i % iter_per_epoch == 0:
        train_acc = network.accuracy(x_train, t_train)
        test_acc = network.accuracy(x_test, t_test)
        train_acc_list.append(train_acc)
        test_acc_list.append(test_acc)
        print("train_acc:{}".format(train_acc), "test_acc:{}".format(test_acc))
        # train_acc:0.09818333333333333 test_acc:0.0972
        # train_acc:0.90195 test_acc:0.9038 
        # ...
        # train_acc:0.9774 test_acc:0.9687
        # train_acc:0.9784333333333334 test_acc:0.9706
```



### 3. 수치미분을 통해 오차역전파법을 통해 구한 기울기 검증

```python
import sys, os
sys.path.append("./code/")
sys.path.append("./code/ch05/")

import numpy as np
from dataset.mnist import load_mnist # mnist 데이터 관련 함수
from two_layer_net import TwoLayerNet # 기울기 구하는 함수

# 데이터 읽기
(x_train, t_train), (x_test, t_test) = load_mnist(normalize=True, one_hot_label=True)

network = TwoLayerNet(input_size=784, hidden_size=50, output_size=10)

x_batch = x_train[:3]
t_batch = t_train[:3]

grad_numerical = network.numerical_gradient(x_batch, t_batch)
grad_backprop = network.gradient(x_batch, t_batch)

# 각 가중치의 절대 오차의 평균을 구한다.
for key in grad_numerical.keys():
    diff = np.average( np.abs(grad_backprop[key] - grad_numerical[key]) )
    print(key + ":" + str(diff))
```



---

그림 출처: 밑바닥부터 시작하는 딥러닝
