# 220322 ğŸ•



## í•™ìŠµ ì•Œê³ ë¦¬ì¦˜ êµ¬í˜„



### 1. ì‹ ê²½ë§ í•™ìŠµì´ë€?

í•™ìŠµ : ìµœì í™”ëœ weightsë¥¼ êµ¬í•˜ëŠ” ê³¼ì •

- ìˆœì „íŒŒ: input -> output (ì¶”ë¡ ì˜ ê³¼ì •)

- **ì—­ì „íŒŒ (Backpropagation)** : output -> inputìœ¼ë¡œ ê°€ë©´ì„œ weightê°’ì„ update (í•™ìŠµì˜ ê³¼ì •)
  - ì´ë•Œ, inputì´ 1ê°œë¼ë©´, ë‹¤ë¥¸ inputê°’ë“¤ì„ ê³ ë ¤í•˜ì§€ ì•Šê³  weightë¥¼ êµ¬í•˜ëŠ” ê²ƒ -> ì¼ë°˜í™”ê°€ ì•ˆë¨
  - ë”°ë¼ì„œ, ë°°ì¹˜ì²˜ë¦¬ë¥¼ í†µí•´ inputì— ì—¬ëŸ¬ ê°’ì„ ë†“ê³  í•œë²ˆì— weightë¥¼ êµ¬í•¨.



**ì‹ ê²½ë§ í•™ìŠµ -> ë°ì´í„° ì£¼ë„ í•™ìŠµ**

- end to end ê¸°ê³„í•™ìŠµ
  - ê¸°ì¡´ì— ì‚¬ëŒì´ í•˜ë˜ feature engineering ë“±ì„ ì¼ì¼ì´ í•˜ë˜ ê²ƒê³¼ ë‹¬ë¦¬, ì‚¬ëŒì˜ ê°œì…ì´ ê±°ì˜ ì—†ì–´ì§„ í•™ìŠµ (ë°ì´í„°ë§Œ ë„£ìœ¼ë©´ ì»´í“¨í„°ê°€ ì•Œì•„ì„œ ì§„í–‰)
  - ê·œì¹™ì„ 'ì‚¬ëŒ'ì´ ë§Œë“œëŠ” ë°©ì‹ì—ì„œ **'ê¸°ê³„'ê°€ ë°ì´í„°ë¡œë¶€í„° ë°°ìš°ëŠ” ë°©ì‹**ìœ¼ë¡œì˜ íŒ¨ëŸ¬ë‹¤ì„ ì „í™˜
- **í›ˆë ¨ ë°ì´í„°, ì‹œí—˜ ë°ì´í„°** 
  - í›ˆë ¨ ë°ì´í„° (train): í•™ìŠµ ë°ì´í„°
  - ê²€ì¦ ë°ì´í„° (validation): ëª¨ë¸ì˜ ì„±ëŠ¥ì„ ê²€ì¦í•˜ê¸° ìœ„í•´ ì„±ëŠ¥ ì§€í‘œë¥¼ ë³´ê¸° ìœ„í•œ ë°ì´í„°
  - ì‹œí—˜ ë°ì´í„° (test): ì‹¤ì œ ëª¨ë¸ì„ ì ìš©í•˜ëŠ” ë°ì´í„°
  - Over Fitting : í•œ ë°ì´í„° ì…‹ì—ë§Œ ì§€ë‚˜ì¹˜ê²Œ ìµœì í™”ëœ ìƒíƒœ



### 2. í•™ìŠµì˜ ê¸°ì¤€? ì†ì‹¤í•¨ìˆ˜

- **loss function** (cost function, ë¹„ìš©í•¨ìˆ˜)
  - ì‹ ê²½ë§ ì„±ëŠ¥ì˜ 'ë‚˜ì¨'ì„ ë‚˜íƒ€ë‚´ëŠ” ì§€í‘œ. í›ˆë ¨ ë°ì´í„°ë¥¼ ì–¼ë§ˆë‚˜ ëª» ì²˜ë¦¬í•˜ëŠëƒ?
  - error: ì˜ˆì¸¡ê°’ì„ ì •ë‹µê³¼ ë¹„êµí–ˆì„ ë•Œ ì–¼ë§ˆë‚˜ í‹€ë ¸ëŠ”ì§€?

- í•™ìŠµì˜ ê³¼ì •ì—ì„œ **ì†ì‹¤í•¨ìˆ˜ì˜ ê°’ì„ ìµœì†Œí™”**í•˜ëŠ” ìµœì  ê°€ì¤‘ì¹˜ë¥¼ ì°¾ì•„ì•¼ í•¨
  - ìˆ˜ì¹˜ë¯¸ë¶„/ì˜¤ì°¨ ì—­ì „íŒŒë²•ì„ í†µí•´ **ê²½ì‚¬í•˜ê°•ë²•(G.D)** ì‹¤í–‰í•˜ì—¬ ê°€ì¤‘ì¹˜ë¥¼ Update



#### 1) MSE (í‰ê· ì œê³±ì˜¤ì°¨)

- **LR(ì„ í˜•íšŒê·€)**ì˜ ì†ì‹¤í•¨ìˆ˜ -> MSE(í‰ê· ì œê³±ì˜¤ì°¨)

![image](https://user-images.githubusercontent.com/100326309/160083104-b72ff616-39ea-4350-a398-5912705dc17d.png)

- y: ì‹ ê²½ë§ì´ ì¶”ì •í•œ ê°’
- t: ì •ë‹µ ë ˆì´ë¸”
- k: ë°ì´í„°ì˜ ì°¨ì› ìˆ˜

```PYTHON
# MSE í•¨ìˆ˜ ìƒì„±
import numpy as np
def mean_squared_error(y, t):
    return 0.5 * np.sum((y - t) ** 2)

# ë„ì¶œ ì˜ˆì‹œ
y = [0.1, 0.05, 0.6, 0.0, 0.05, 0.1, 0.0, 0.1, 0.0, 0.0]
t = [0, 0, 1, 0, 0, 0, 0, 0, 0, 0] # ì •ë‹µì— í•´ë‹¹í•˜ëŠ” ì›ì†Œë§Œ 1 (ì›-í•« ì¸ì½”ë”© ë°©ì‹)

mean_squared_error(np.array(y),np.array(t)) # 0.09750000000000003
```



#### 2) êµì°¨ ì—”íŠ¸ë¡œí”¼ ì˜¤ì°¨

- **Logistic Regression**ì˜ ì†ì‹¤í•¨ìˆ˜ -> êµì°¨ ì—”íŠ¸ë¡œí”¼ ì˜¤ì°¨

![image](https://user-images.githubusercontent.com/100326309/160083244-e0f791a8-4ebe-4fbe-ab61-357c61c349a6.png)

- log : ë°‘ì´ eì¸ ìì—°ë¡œê·¸(ln)
- y: ì‹ ê²½ë§ì´ ì¶”ì •í•œ ê°’
- t: ì •ë‹µ ë ˆì´ë¸” (t = 1ì¼ ë•Œë§Œ ìì—°ë¡œê·¸ë¥¼ ê³„ì‚°)
- k: ë°ì´í„°ì˜ ì°¨ì› ìˆ˜
- ë§¨ ì• ìŒìˆ˜:  ì˜¤ì°¨ë¥¼ ì–‘ìˆ˜ë¡œ ë§Œë“¤ì–´ ì£¼ê¸° ìœ„í•´

```python
# êµì°¨ ì—”íŠ¸ë¡œí”¼ ì˜¤ì°¨ í•¨ìˆ˜ ìƒì„±
import numpy as np
def cross_entropy_error(y, t):
    delta = 1e-7 # ì—¬ê¸°ì„œ e = 10, 1e-7 = 1/10**7
    return -np.sum(t * np.log(y + delta)) # np.log() í•¨ìˆ˜ì— 0ì„ ì…ë ¥í•˜ë©´ ë§ˆì´ë„ˆìŠ¤ ë¬´í•œëŒ€ê°€ ë˜ë¯€ë¡œ ê³„ì‚° ì§„í–‰x
                                          # ë”°ë¼ì„œ 0ì´ ë˜ì§€ ì•Šê¸° ìœ„í•´ deltaë¥¼ ë”í•¨
    
# ë„ì¶œ ì˜ˆì‹œ
y = [0.1, 0.05, 0.6, 0.0, 0.05, 0.1, 0.0, 0.1, 0.0, 0.0]
t = [0, 0, 1, 0, 0, 0, 0, 0, 0, 0] # ì •ë‹µì— í•´ë‹¹í•˜ëŠ” ì›ì†Œë§Œ 1 (ì›-í•« ì¸ì½”ë”© ë°©ì‹)

cross_entropy_error(np.array(y), np.array(t)) # 0.510825457099338
```

- ì‹ ê²½ë§ì˜ ê²°ê³¼ê°’ì´ ì •ë‹µ(1)ì— ê°€ê¹ê²Œ ë‚˜íƒ€ë‚ ê²½ìš° ì˜¤ì°¨ ê°’ì´ ì¤„ì–´ë“¤ê³ , 0ì— ê°€ê¹ê²Œ ë‚˜íƒ€ë‚ ìˆ˜ë¡ ì˜¤ì°¨ëŠ” ì»¤ì§„ë‹¤.

- **ë¯¸ë‹ˆë°°ì¹˜ í•™ìŠµì—ì„œì˜ êµì°¨ ì—”íŠ¸ë¡œí”¼ ì˜¤ì°¨**
  - ê° í›ˆë ¨ ë°ì´í„°ë§ˆë‹¤ì˜ ì†ì‹¤í•¨ìˆ˜ë¥¼ êµ¬í•´ **í‰ê· ì†ì‹¤í•¨ìˆ˜**ë¥¼ ê³„ì‚°í•  ë•Œ, ì‹œê°„ì„ ë‹¨ì¶•í•˜ê¸° ìœ„í•´ **ë¯¸ë‹ˆë°°ì¹˜ í•™ìŠµ**ì„ ìˆ˜í–‰

```python
# ë°°ì¹˜ë°ì´í„°ë¥¼ ì§€ì›í•˜ëŠ” êµì°¨ ì—”íŠ¸ë¡œí”¼ ì˜¤ì°¨ êµ¬í•˜ê¸°
def cross_entropy_error_1(y, t):
    if y.ndim == 1:
        t = t.reshape(1, t.size)
        y = y.reshape(1, t.size)
    batch_size = y.shape[0]
    delta = 1e-7
    return -np.sum(t * np.log(y + delta)) / batch_size

# ë„ì¶œ ì˜ˆì‹œ
y = [0.1, 0.05, 0.6, 0.0, 0.05, 0.1, 0.0, 0.1, 0.0, 0.0]
t = [0, 0, 1, 0, 0, 0, 0, 0, 0, 0] # ì •ë‹µì— í•´ë‹¹í•˜ëŠ” ì›ì†Œë§Œ 1 (ì›-í•« ì¸ì½”ë”© ë°©ì‹)

cross_entropy_error_1(np.array(y), np.array(t)) # 0.510825457099338
```



### 3. ê²½ì‚¬ í•˜ê°•ë²•

- í•™ìŠµì„ ì§„í–‰í•  ë•Œ, ì†ì‹¤í•¨ìˆ˜ì˜ ìµœì†Ÿê°’ì„ ì°¾ëŠ” ë°©ë²•
- í•¨ìˆ˜ì˜ ìµœì†Ÿê°’: ì ‘ì„ ì˜ ê¸°ìš¸ê¸°ê°€ 0ì¸ ì§€ì 

#### 1) ìˆ˜ì¹˜ë¯¸ë¶„

- ì»´í“¨í„° = ê³„ì‚°ê¸° (ì—°ì‚°ì˜ ê¸°ëŠ¥) -> ì»´í“¨í„°ê°€ ë¯¸ë¶„ì„ ì–´ë–»ê²Œ í•˜ì§€? 

- í•´ì„ì  ë¯¸ë¶„: ì¸ê°„ì€ ë„í•¨ìˆ˜ë¥¼ êµ¬í•´ì„œ ë¯¸ë¶„ 

- ìˆ˜ì¹˜ ë¯¸ë¶„: ì»´í“¨í„°ë¡œ ë¯¸ë¶„ì„ í•˜ê¸° ìœ„í•´ì„œëŠ” 0ì— ê°€ê¹Œìš´ ì•„ì£¼ ì‘ì€ ê°’ì„ ëŒ€ì…í•˜ì—¬ ê·¼ì‚¿ê°’ì„ êµ¬í•¨

- ë¯¸ë¶„ê³„ìˆ˜: ìˆœê°„ ë³€í™”ìœ¨ 

![image](https://user-images.githubusercontent.com/100326309/160083267-c876bb7c-2144-49eb-be2f-4038e7281f8e.png)

-> ì¦‰, ìˆ˜ì¹˜ë¯¸ë¶„: ë¯¸ë¶„ê³„ìˆ˜ì˜ ê·¼ì‚¿ê°’ì„ êµ¬í•˜ëŠ” ê²ƒ

```python
# ìˆ˜ì¹˜ë¯¸ë¶„ í•¨ìˆ˜ ì •ì˜
def numerical_diff(f,x):
    h=1e-4
    return (f(x+h)-f(x))/h

# 1ë³€ìˆ˜ í•¨ìˆ˜ ìƒì„±
def function_2(x):
    return 0.01*x**2+0.1*x

# ê·¸ë˜í”„ ê·¸ë ¤ë³´ê¸°
import numpy as np
import matplotlib.pylab as plt
x=np.arange(0.0,20,0.01)
y=function_2(x)
plt.plot(x,y)
plt.show()

# x = 5ì¸ ì§€ì ì— ëŒ€í•œ ë¯¸ë¶„
numerical_diff(function_2,5) # 0.20000099999917254
# x = 10ì¸ ì§€ì ì— ëŒ€í•œ ë¯¸ë¶„
numerical_diff(function_2,10) # 0.3000009999976072


# í•¨ìˆ˜ì— ì ‘ì„  ê·¸ë ¤ë³´ê¸°
def tangent_line(f, x):
    d = numerical_diff(f, x)
    print(d)
    y = f(x) - d*x
    return lambda t: d*t + y
     
x = np.arange(0.0, 20.0, 0.1)
y = function_1(x)
plt.xlabel("x")
plt.ylabel("f(x)")

tf = tangent_line(function_1, 5)
y2 = tf(x)

plt.plot(x, y)
plt.plot(x, y2)
plt.show()
```

![image](https://user-images.githubusercontent.com/100326309/160083315-66056894-c809-47eb-b495-dc30712a0c85.png)

- í¸ë¯¸ë¶„ : ë³€ìˆ˜ê°€ ì—¬ëŸ¿ì¸ í•¨ìˆ˜ì— ëŒ€í•œ ë¯¸ë¶„

  ![image](https://user-images.githubusercontent.com/100326309/160083347-a16b2598-e694-48ec-88d5-d3ff34cd57c9.png)

  ![image](https://user-images.githubusercontent.com/100326309/160083368-4f8722c7-a762-4814-8252-f809a7794557.png)

  - x0, x1ì´ë¼ëŠ” ë‘ ë³€ìˆ˜ë¥¼ ê°€ì§„ ìœ„ì˜ í•¨ìˆ˜ë¥¼ ë¯¸ë¶„í•˜ë©´, x0ì— ëŒ€í•œ ë¯¸ë¶„, x1ì— ëŒ€í•œ ë¯¸ë¶„ ê°ê°ì˜ í•¨ìˆ˜ê°€ ë‚˜ì˜´

```python
# 2ë³€ìˆ˜ í•¨ìˆ˜ ìƒì„±
def function_3(x,y):
    return x**2+y**2

x_3=np.arange(-5,5,0.25)
y_3=np.arange(-5,5,0.25)

function_3(x_3,y_3)

# 2ë³€ìˆ˜ í•¨ìˆ˜ ê·¸ë˜í”„ ê·¸ë¦¬ê¸°
import matplotlib.pyplot as plt
from mpl_toolkits.mplot3d import Axes3D
import numpy as np

fig = plt.figure()
ax = fig.gca(projection='3d')

X,Y = np.meshgrid(x_3,y_3)
Z = function_3(X,Y)

surf = ax.plot_surface(X,Y,Z,cmap='coolwarm',linewidth=0,antialiased=False)
wire = ax.plot_wireframe(X,Y,Z,color='r',linewidth=0.1)
fig.colorbar(surf,shrink=0.5,aspect=5)
fig.tight_layout()
plt.show()
```

![image](https://user-images.githubusercontent.com/100326309/160083537-04cdff92-5f9f-474b-9315-a2ea09321af5.png)

#### 2) ê¸°ìš¸ê¸° (Gradient)

- ë¯¸ë¶„ê°’: í•¨ìˆ˜ê°€ ì–´ë–»ê²Œ ì§„í–‰ë ì§€ ì•Œë ¤ì£¼ëŠ” ì§€í‘œ

- 2ì°¨ì›ì—ì„œì˜ ë¯¸ë¶„ ê²°ê³¼: ê³¡ì„  ìœ„ì˜ ì ‘ì ì˜ ê¸°ìš¸ê¸°
- 3ì°¨ì›ì—ì„œì˜ ë¯¸ë¶„ ê²°ê³¼: ê³¡ì„  ìœ„ì˜ ì ‘í‰ë©´ì˜ ë²•ì„  ë°©í–¥(ë²•ì„ ë²¡í„°)



#### 3) ê²½ì‚¬ í•˜ê°•ë²• (ê²½ì‚¬ë²•, GD)

- ê·¹ì†Ÿê°’(local minimum)ì„ ì°¾ëŠ” ì•Œê³ ë¦¬ì¦˜
  - But, ì‹¤ì œë¡œ ìµœì†Ÿê°’ì„ì„ ë³´ì¥í•˜ê¸°ëŠ” ì–´ë ¤ì›€

- í•¨ìˆ˜ì˜ ê¸°ìš¸ê¸°(ê²½ì‚¬)ë¥¼ êµ¬í•˜ì—¬ **ê¸°ìš¸ê¸°ê°€ ë‚®ì€ ìª½ìœ¼ë¡œ ê³„ì† ì´ë™**ì‹œì¼œì„œ ê·¹ê°’ì— ì´ë¥¼ ë•Œê¹Œì§€ ë°˜ë³µ -> ê¸°ìš¸ê¸° ê°’ì´ 0ì´ ë  ë•Œê¹Œì§€!

![image](https://user-images.githubusercontent.com/100326309/160083561-0ecbda0c-fa57-4ed0-9e1c-11b13f3d384f.png)

- Î·(ì—íƒ€): ê°±ì‹ í•˜ëŠ” ì–‘ (ê±°ë¦¬ë¥¼ ì¡°ì •)ì„ ë‚˜íƒ€ëƒ„.

  = LR (í•™ìŠµë¥ , Learning Rate)	

- ì‹ ê²½ë§ í•™ìŠµì—ì„œì˜ ê¸°ìš¸ê¸° êµ¬í•˜ê¸°

  - SGD(Stochastic Gradient Descent): í™•ë¥ ì  ê²½ì‚¬í•˜ê°•ë²•, ë¯¸ë‹ˆë°°ì¹˜ì²˜ë¦¬ í›„ ê²½ì‚¬ë²• ì ìš©

  - simpleNet Class ì‹¤ìŠµ

    ```python
    import sys, os
    sys.path.append("./code/")  
    import numpy as np
    from common.functions import softmax, cross_entropy_error
    from common.gradient import numerical_gradient
    
    # simpleNet Class
    class simpleNet:
        # ê°€ì¤‘ì¹˜ ì´ˆê¸°ê°’ì„ (2*3)í˜•íƒœë¡œ ëœëŸ¼ ì§€ì • í›„ ìˆ˜í–‰
        def __init__(self):
            self.W = np.random.randn(2,3) 
        
        # ì˜ˆì¸¡ ìˆ˜í–‰ í•¨ìˆ˜   
        def predict(self,x): 
            return np.dot(x, self.W)
        
        # ì†ì‹¤ê°’ì„ êµ¬í•˜ëŠ” í•¨ìˆ˜ (êµì°¨ ì—”íŠ¸ë¡œí”¼ ì˜¤ì°¨ ì´ìš©)
        def loss(self, x, t): # 
            z = self.predict(x)
            y = softmax(z)
            loss = cross_entropy_error(y,t)
            
            return loss
    
    ## ì˜ˆì¸¡ ìˆ˜í–‰í•´ë³´ê¸°
    net = simpleNet()
    # ê°€ì¤‘ì¹˜ ê¸°ë³¸ê°’ ì¶œë ¥
    print(net.W) # [[ 0.71338084 -0.94925867  0.32417468]
                 # [-0.58673328 -0.36919303  1.15482563]]
    
    # (0.6, 0.9)ë¥¼ ê°€ì¤‘ì¹˜ ê¸°ë³¸ê°’ê³¼ í–‰ë ¬ê³±
    x = np.array([0.6,0.9])
    t = np.array([0,0,1]) 
    p = net.predict(x)
    print(p) # [-0.10003145 -0.90182893  1.23384788]
    
    # xì™€ tì˜ ì†ì‹¤ê°’ êµ¬í•˜ê¸°
    net.loss(x,t) # 0.3232550250958044
    ```

    

### 4. ì‹ ê²½ë§ í•™ìŠµ

**1ë‹¨ê³„) ë¯¸ë‹ˆë°°ì¹˜**

í›ˆë ¨ ë°ì´í„° ì¤‘ ì¼ë¶€ë¥¼ ë¬´ì‘ìœ„ë¡œ ê°€ì ¸ì˜´. ì´ ë¯¸ë‹ˆë°°ì¹˜ì˜ ì†ì‹¤í•¨ìˆ˜ ê°’ì„ ì¤„ì´ëŠ” ê²ƒì´ ëª©í‘œ



**2ë‹¨ê³„) ê¸°ìš¸ê¸° ì‚°ì¶œ**

ë¯¸ë‹ˆë°°ì¹˜ì˜ ì†ì‹¤í•¨ìˆ˜ ê°’ì„ ì¤„ì´ê¸° ìœ„í•´ ê° ê°€ì¤‘ì¹˜ ë§¤ê°œë³€ìˆ˜ì˜ ê¸°ìš¸ê¸°ë¥¼ êµ¬í•¨. 



**3ë‹¨ê³„) ë§¤ê°œë³€ìˆ˜ ê°±ì‹ **

ê°€ì¤‘ì¹˜ ë§¤ê°œë³€ìˆ˜ë¥¼ ê¸°ìš¸ê¸° ë°©í–¥ìœ¼ë¡œ ì•„ì£¼ ì¡°ê¸ˆ ê°±ì‹ 



**4ë‹¨ê³„) ë°˜ë³µ**

1~3ë‹¨ê³„ì˜ ë°˜ë³µ

- **ì—í­(epoch)**
  - 1ì—í­: í•™ìŠµì—ì„œ í›ˆë ¨ ë°ì´í„°ë¥¼ ëª¨ë‘ ì†Œì§„í–ˆì„ ë•Œì˜ íšŸìˆ˜
  - ex) 10,000ê°œë¥¼ 100ê°œì˜ ë¯¸ë‹ˆë°°ì¹˜ë¡œ í•™ìŠµí•  ê²½ìš°, í™•ë¥ ì  ê²½ì‚¬ í•˜ê°•ë²•ì„ 100íšŒ ë°˜ë³µí•˜ë©´ ëª¨ë“  í›ˆë ¨ ë°ì´í„°ë¥¼ 'ì†Œì§„'  -> ì´ ê²½ìš° 100íšŒê°€ 1ì—í­

```PYTHON
# ì‹ ê²½ë§ í´ë˜ìŠ¤ êµ¬í˜„
import sys, os
sys.path.append("./code/") 
from common.functions import *
from common.gradient import numerical_gradient
import numpy as np


class TwoLayerNet:

    def __init__(self, input_size, hidden_size, output_size, weight_init_std=0.01):
        # ê°€ì¤‘ì¹˜ ì´ˆê¸°í™”
        self.params = {}
        self.params['W1'] = weight_init_std * np.random.randn(input_size, hidden_size)
        self.params['b1'] = np.zeros(hidden_size)
        self.params['W2'] = weight_init_std * np.random.randn(hidden_size, output_size)
        self.params['b2'] = np.zeros(output_size)

    def predict(self, x):
        W1, W2 = self.params['W1'], self.params['W2']
        b1, b2 = self.params['b1'], self.params['b2']
    
        a1 = np.dot(x, W1) + b1
        z1 = sigmoid(a1)
        a2 = np.dot(z1, W2) + b2
        y = softmax(a2)
        
        return y
        
    # x : ì…ë ¥ ë°ì´í„°, t : ì •ë‹µ ë ˆì´ë¸”
    def loss(self, x, t):
        y = self.predict(x)
        
        return cross_entropy_error(y, t)
    
    def accuracy(self, x, t):
        y = self.predict(x)
        y = np.argmax(y, axis=1)
        t = np.argmax(t, axis=1)
        
        accuracy = np.sum(y == t) / float(x.shape[0])
        return accuracy
        
    # x : ì…ë ¥ ë°ì´í„°, t : ì •ë‹µ ë ˆì´ë¸”
    def numerical_gradient(self, x, t):
        loss_W = lambda W: self.loss(x, t)
        
        grads = {}
        grads['W1'] = numerical_gradient(loss_W, self.params['W1'])
        grads['b1'] = numerical_gradient(loss_W, self.params['b1'])
        grads['W2'] = numerical_gradient(loss_W, self.params['W2'])
        grads['b2'] = numerical_gradient(loss_W, self.params['b2'])
        
        return grads
        
    def gradient(self, x, t):
        W1, W2 = self.params['W1'], self.params['W2']
        b1, b2 = self.params['b1'], self.params['b2']
        grads = {}
        
        batch_num = x.shape[0]
        
        # forward
        a1 = np.dot(x, W1) + b1
        z1 = sigmoid(a1)
        a2 = np.dot(z1, W2) + b2
        y = softmax(a2)
        
        # backward
        dy = (y - t) / batch_num
        grads['W2'] = np.dot(z1.T, dy)
        grads['b2'] = np.sum(dy, axis=0)
        
        da1 = np.dot(dy, W2.T)
        dz1 = sigmoid_grad(a1) * da1
        grads['W1'] = np.dot(x.T, dz1)
        grads['b1'] = np.sum(dz1, axis=0)

        return grads

# ë¯¸ë‹ˆë°°ì¹˜ í•™ìŠµ êµ¬í˜„
import matplotlib.pyplot as plt
from dataset.mnist import load_mnist

# ë°ì´í„° ì½ê¸°
(x_train, t_train), (x_test, t_test) = load_mnist(normalize=True, one_hot_label=True)

network = TwoLayerNet(input_size=784, hidden_size=50, output_size=10)

# í•˜ì´í¼íŒŒë¼ë¯¸í„°
iters_num = 10000  # ë°˜ë³µ íšŸìˆ˜ë¥¼ ì ì ˆíˆ ì„¤ì •í•œë‹¤.
train_size = x_train.shape[0]
batch_size = 100   # ë¯¸ë‹ˆë°°ì¹˜ í¬ê¸°
learning_rate = 0.1

train_loss_list = []
train_acc_list = []
test_acc_list = []

# 1ì—í­ë‹¹ ë°˜ë³µ ìˆ˜
iter_per_epoch = max(train_size / batch_size, 1)

for i in range(iters_num):
    # ë¯¸ë‹ˆë°°ì¹˜ íšë“
    batch_mask = np.random.choice(train_size, batch_size)
    x_batch = x_train[batch_mask]
    t_batch = t_train[batch_mask]
    # ê¸°ìš¸ê¸° ê³„ì‚°
    #grad = network.numerical_gradient(x_batch, t_batch)
    grad = network.gradient(x_batch, t_batch)
    
    # ë§¤ê°œë³€ìˆ˜ ê°±ì‹ 
    for key in ('W1', 'b1', 'W2', 'b2'):
        network.params[key] -= learning_rate * grad[key]
    
    # í•™ìŠµ ê²½ê³¼ ê¸°ë¡
    loss = network.loss(x_batch, t_batch)
    train_loss_list.append(loss)
    
    # 1ì—í­ë‹¹ ì •í™•ë„ ê³„ì‚°
    if i % iter_per_epoch == 0:
        train_acc = network.accuracy(x_train, t_train)
        test_acc = network.accuracy(x_test, t_test)
        train_acc_list.append(train_acc)
        test_acc_list.append(test_acc)
        print("train acc, test acc | " + str(train_acc) + ", " + str(test_acc))
    
    

# ê·¸ë˜í”„ ê·¸ë¦¬ê¸°
markers = {'train': 'o', 'test': 's'}
x = np.arange(len(train_acc_list))
plt.plot(x, train_acc_list, label='train acc')
plt.plot(x, test_acc_list, label='test acc', linestyle='--')
plt.xlabel("epochs")
plt.ylabel("accuracy")
plt.ylim(0, 1.0)
plt.legend(loc='lower right')
plt.show()

markers = {'train': 'o', 'test': 's'}
x = np.arange(len(train_loss_list))
plt.plot(x, train_loss_list, label='train loss')
plt.xlabel("internation")
plt.ylabel("loss")
plt.ylim(0, 9.0)
plt.legend(loc='lower right')
plt.show()
```

![image](https://user-images.githubusercontent.com/100326309/160083695-27e3b9e5-a5e0-48f8-80a5-2ca695568da0.png)

![image](https://user-images.githubusercontent.com/100326309/160083651-0405c77d-19a3-4015-adf7-9b0f7294c9e7.png)

- ê·¸ë˜í”„ í•´ì„
  - í•™ìŠµ íšŸìˆ˜(epochs)ì´ ëŠ˜ì–´ê°€ë©´ì„œ ì •í™•ë„(accuarcy)ëŠ” ë†’ì•„ì§€ê³  ì†ì‹¤ í•¨ìˆ˜(loss)ì˜ ê°’ì´ ì¤„ì–´ë“ ë‹¤.
  - ë§¤ê°œë³€ìˆ˜ê°€ ì„œì„œíˆ ë°ì´í„°ì— ì ì‘í•˜ê³  ìˆìŒì„ ì˜ë¯¸í•œë‹¤.
  
  

---

ê·¸ë¦¼ ì¶œì²˜: ë°‘ë°”ë‹¥ë¶€í„° ì‹œì‘í•˜ëŠ” ë”¥ëŸ¬ë‹
