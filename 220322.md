# 220322 🍕



## 학습 알고리즘 구현



### 1. 신경망 학습이란?

학습 : 최적화된 weights를 구하는 과정

- 순전파: input -> output (추론의 과정)

- **역전파 (Backpropagation)** : output -> input으로 가면서 weight값을 update (학습의 과정)
  - 이때, input이 1개라면, 다른 input값들을 고려하지 않고 weight를 구하는 것 -> 일반화가 안됨
  - 따라서, 배치처리를 통해 input에 여러 값을 놓고 한번에 weight를 구함.



**신경망 학습 -> 데이터 주도 학습**

- end to end 기계학습
  - 기존에 사람이 하던 feature engineering 등을 일일이 하던 것과 달리, 사람의 개입이 거의 없어진 학습 (데이터만 넣으면 컴퓨터가 알아서 진행)
  - 규칙을 '사람'이 만드는 방식에서 **'기계'가 데이터로부터 배우는 방식**으로의 패러다임 전환
- **훈련 데이터, 시험 데이터** 
  - 훈련 데이터 (train): 학습 데이터
  - 검증 데이터 (validation): 모델의 성능을 검증하기 위해 성능 지표를 보기 위한 데이터
  - 시험 데이터 (test): 실제 모델을 적용하는 데이터
  - Over Fitting : 한 데이터 셋에만 지나치게 최적화된 상태



### 2. 학습의 기준? 손실함수

- **loss function** (cost function, 비용함수)
  - 신경망 성능의 '나쁨'을 나타내는 지표. 훈련 데이터를 얼마나 못 처리하느냐?
  - error: 예측값을 정답과 비교했을 때 얼마나 틀렸는지?

- 학습의 과정에서 **손실함수의 값을 최소화**하는 최적 가중치를 찾아야 함
  - 수치미분/오차 역전파법을 통해 **경사하강법(G.D)** 실행하여 가중치를 Update



#### 1) MSE (평균제곱오차)

- **LR(선형회귀)**의 손실함수 -> MSE(평균제곱오차)

![image](https://user-images.githubusercontent.com/100326309/160083104-b72ff616-39ea-4350-a398-5912705dc17d.png)

- y: 신경망이 추정한 값
- t: 정답 레이블
- k: 데이터의 차원 수

```PYTHON
# MSE 함수 생성
import numpy as np
def mean_squared_error(y, t):
    return 0.5 * np.sum((y - t) ** 2)

# 도출 예시
y = [0.1, 0.05, 0.6, 0.0, 0.05, 0.1, 0.0, 0.1, 0.0, 0.0]
t = [0, 0, 1, 0, 0, 0, 0, 0, 0, 0] # 정답에 해당하는 원소만 1 (원-핫 인코딩 방식)

mean_squared_error(np.array(y),np.array(t)) # 0.09750000000000003
```



#### 2) 교차 엔트로피 오차

- **Logistic Regression**의 손실함수 -> 교차 엔트로피 오차

![image](https://user-images.githubusercontent.com/100326309/160083244-e0f791a8-4ebe-4fbe-ab61-357c61c349a6.png)

- log : 밑이 e인 자연로그(ln)
- y: 신경망이 추정한 값
- t: 정답 레이블 (t = 1일 때만 자연로그를 계산)
- k: 데이터의 차원 수
- 맨 앞 음수:  오차를 양수로 만들어 주기 위해

```python
# 교차 엔트로피 오차 함수 생성
import numpy as np
def cross_entropy_error(y, t):
    delta = 1e-7 # 여기서 e = 10, 1e-7 = 1/10**7
    return -np.sum(t * np.log(y + delta)) # np.log() 함수에 0을 입력하면 마이너스 무한대가 되므로 계산 진행x
                                          # 따라서 0이 되지 않기 위해 delta를 더함
    
# 도출 예시
y = [0.1, 0.05, 0.6, 0.0, 0.05, 0.1, 0.0, 0.1, 0.0, 0.0]
t = [0, 0, 1, 0, 0, 0, 0, 0, 0, 0] # 정답에 해당하는 원소만 1 (원-핫 인코딩 방식)

cross_entropy_error(np.array(y), np.array(t)) # 0.510825457099338
```

- 신경망의 결과값이 정답(1)에 가깝게 나타날경우 오차 값이 줄어들고, 0에 가깝게 나타날수록 오차는 커진다.

- **미니배치 학습에서의 교차 엔트로피 오차**
  - 각 훈련 데이터마다의 손실함수를 구해 **평균손실함수**를 계산할 때, 시간을 단축하기 위해 **미니배치 학습**을 수행

```python
# 배치데이터를 지원하는 교차 엔트로피 오차 구하기
def cross_entropy_error_1(y, t):
    if y.ndim == 1:
        t = t.reshape(1, t.size)
        y = y.reshape(1, t.size)
    batch_size = y.shape[0]
    delta = 1e-7
    return -np.sum(t * np.log(y + delta)) / batch_size

# 도출 예시
y = [0.1, 0.05, 0.6, 0.0, 0.05, 0.1, 0.0, 0.1, 0.0, 0.0]
t = [0, 0, 1, 0, 0, 0, 0, 0, 0, 0] # 정답에 해당하는 원소만 1 (원-핫 인코딩 방식)

cross_entropy_error_1(np.array(y), np.array(t)) # 0.510825457099338
```



### 3. 경사 하강법

- 학습을 진행할 때, 손실함수의 최솟값을 찾는 방법
- 함수의 최솟값: 접선의 기울기가 0인 지점

#### 1) 수치미분

- 컴퓨터 = 계산기 (연산의 기능) -> 컴퓨터가 미분을 어떻게 하지? 

- 해석적 미분: 인간은 도함수를 구해서 미분 

- 수치 미분: 컴퓨터로 미분을 하기 위해서는 0에 가까운 아주 작은 값을 대입하여 근삿값을 구함

- 미분계수: 순간 변화율 

![image](https://user-images.githubusercontent.com/100326309/160083267-c876bb7c-2144-49eb-be2f-4038e7281f8e.png)

-> 즉, 수치미분: 미분계수의 근삿값을 구하는 것

```python
# 수치미분 함수 정의
def numerical_diff(f,x):
    h=1e-4
    return (f(x+h)-f(x))/h

# 1변수 함수 생성
def function_2(x):
    return 0.01*x**2+0.1*x

# 그래프 그려보기
import numpy as np
import matplotlib.pylab as plt
x=np.arange(0.0,20,0.01)
y=function_2(x)
plt.plot(x,y)
plt.show()

# x = 5인 지점에 대한 미분
numerical_diff(function_2,5) # 0.20000099999917254
# x = 10인 지점에 대한 미분
numerical_diff(function_2,10) # 0.3000009999976072


# 함수에 접선 그려보기
def tangent_line(f, x):
    d = numerical_diff(f, x)
    print(d)
    y = f(x) - d*x
    return lambda t: d*t + y
     
x = np.arange(0.0, 20.0, 0.1)
y = function_1(x)
plt.xlabel("x")
plt.ylabel("f(x)")

tf = tangent_line(function_1, 5)
y2 = tf(x)

plt.plot(x, y)
plt.plot(x, y2)
plt.show()
```

![image](https://user-images.githubusercontent.com/100326309/160083315-66056894-c809-47eb-b495-dc30712a0c85.png)

- 편미분 : 변수가 여럿인 함수에 대한 미분

  ![image](https://user-images.githubusercontent.com/100326309/160083347-a16b2598-e694-48ec-88d5-d3ff34cd57c9.png)

  ![image](https://user-images.githubusercontent.com/100326309/160083368-4f8722c7-a762-4814-8252-f809a7794557.png)

  - x0, x1이라는 두 변수를 가진 위의 함수를 미분하면, x0에 대한 미분, x1에 대한 미분 각각의 함수가 나옴

```python
# 2변수 함수 생성
def function_3(x,y):
    return x**2+y**2

x_3=np.arange(-5,5,0.25)
y_3=np.arange(-5,5,0.25)

function_3(x_3,y_3)

# 2변수 함수 그래프 그리기
import matplotlib.pyplot as plt
from mpl_toolkits.mplot3d import Axes3D
import numpy as np

fig = plt.figure()
ax = fig.gca(projection='3d')

X,Y = np.meshgrid(x_3,y_3)
Z = function_3(X,Y)

surf = ax.plot_surface(X,Y,Z,cmap='coolwarm',linewidth=0,antialiased=False)
wire = ax.plot_wireframe(X,Y,Z,color='r',linewidth=0.1)
fig.colorbar(surf,shrink=0.5,aspect=5)
fig.tight_layout()
plt.show()
```

![image](https://user-images.githubusercontent.com/100326309/160083537-04cdff92-5f9f-474b-9315-a2ea09321af5.png)

#### 2) 기울기 (Gradient)

- 미분값: 함수가 어떻게 진행될지 알려주는 지표

- 2차원에서의 미분 결과: 곡선 위의 접점의 기울기
- 3차원에서의 미분 결과: 곡선 위의 접평면의 법선 방향(법선벡터)



#### 3) 경사 하강법 (경사법, GD)

- 극솟값(local minimum)을 찾는 알고리즘
  - But, 실제로 최솟값임을 보장하기는 어려움

- 함수의 기울기(경사)를 구하여 **기울기가 낮은 쪽으로 계속 이동**시켜서 극값에 이를 때까지 반복 -> 기울기 값이 0이 될 때까지!

![image](https://user-images.githubusercontent.com/100326309/160083561-0ecbda0c-fa57-4ed0-9e1c-11b13f3d384f.png)

- η(에타): 갱신하는 양 (거리를 조정)을 나타냄.

  = LR (학습률, Learning Rate)	

- 신경망 학습에서의 기울기 구하기

  - SGD(Stochastic Gradient Descent): 확률적 경사하강법, 미니배치처리 후 경사법 적용

  - simpleNet Class 실습

    ```python
    import sys, os
    sys.path.append("./code/")  
    import numpy as np
    from common.functions import softmax, cross_entropy_error
    from common.gradient import numerical_gradient
    
    # simpleNet Class
    class simpleNet:
        # 가중치 초기값을 (2*3)형태로 랜럼 지정 후 수행
        def __init__(self):
            self.W = np.random.randn(2,3) 
        
        # 예측 수행 함수   
        def predict(self,x): 
            return np.dot(x, self.W)
        
        # 손실값을 구하는 함수 (교차 엔트로피 오차 이용)
        def loss(self, x, t): # 
            z = self.predict(x)
            y = softmax(z)
            loss = cross_entropy_error(y,t)
            
            return loss
    
    ## 예측 수행해보기
    net = simpleNet()
    # 가중치 기본값 출력
    print(net.W) # [[ 0.71338084 -0.94925867  0.32417468]
                 # [-0.58673328 -0.36919303  1.15482563]]
    
    # (0.6, 0.9)를 가중치 기본값과 행렬곱
    x = np.array([0.6,0.9])
    t = np.array([0,0,1]) 
    p = net.predict(x)
    print(p) # [-0.10003145 -0.90182893  1.23384788]
    
    # x와 t의 손실값 구하기
    net.loss(x,t) # 0.3232550250958044
    ```

    

### 4. 신경망 학습

**1단계) 미니배치**

훈련 데이터 중 일부를 무작위로 가져옴. 이 미니배치의 손실함수 값을 줄이는 것이 목표



**2단계) 기울기 산출**

미니배치의 손실함수 값을 줄이기 위해 각 가중치 매개변수의 기울기를 구함. 



**3단계) 매개변수 갱신**

가중치 매개변수를 기울기 방향으로 아주 조금 갱신



**4단계) 반복**

1~3단계의 반복

- **에폭(epoch)**
  - 1에폭: 학습에서 훈련 데이터를 모두 소진했을 때의 횟수
  - ex) 10,000개를 100개의 미니배치로 학습할 경우, 확률적 경사 하강법을 100회 반복하면 모든 훈련 데이터를 '소진'  -> 이 경우 100회가 1에폭

```PYTHON
# 신경망 클래스 구현
import sys, os
sys.path.append("./code/") 
from common.functions import *
from common.gradient import numerical_gradient
import numpy as np


class TwoLayerNet:

    def __init__(self, input_size, hidden_size, output_size, weight_init_std=0.01):
        # 가중치 초기화
        self.params = {}
        self.params['W1'] = weight_init_std * np.random.randn(input_size, hidden_size)
        self.params['b1'] = np.zeros(hidden_size)
        self.params['W2'] = weight_init_std * np.random.randn(hidden_size, output_size)
        self.params['b2'] = np.zeros(output_size)

    def predict(self, x):
        W1, W2 = self.params['W1'], self.params['W2']
        b1, b2 = self.params['b1'], self.params['b2']
    
        a1 = np.dot(x, W1) + b1
        z1 = sigmoid(a1)
        a2 = np.dot(z1, W2) + b2
        y = softmax(a2)
        
        return y
        
    # x : 입력 데이터, t : 정답 레이블
    def loss(self, x, t):
        y = self.predict(x)
        
        return cross_entropy_error(y, t)
    
    def accuracy(self, x, t):
        y = self.predict(x)
        y = np.argmax(y, axis=1)
        t = np.argmax(t, axis=1)
        
        accuracy = np.sum(y == t) / float(x.shape[0])
        return accuracy
        
    # x : 입력 데이터, t : 정답 레이블
    def numerical_gradient(self, x, t):
        loss_W = lambda W: self.loss(x, t)
        
        grads = {}
        grads['W1'] = numerical_gradient(loss_W, self.params['W1'])
        grads['b1'] = numerical_gradient(loss_W, self.params['b1'])
        grads['W2'] = numerical_gradient(loss_W, self.params['W2'])
        grads['b2'] = numerical_gradient(loss_W, self.params['b2'])
        
        return grads
        
    def gradient(self, x, t):
        W1, W2 = self.params['W1'], self.params['W2']
        b1, b2 = self.params['b1'], self.params['b2']
        grads = {}
        
        batch_num = x.shape[0]
        
        # forward
        a1 = np.dot(x, W1) + b1
        z1 = sigmoid(a1)
        a2 = np.dot(z1, W2) + b2
        y = softmax(a2)
        
        # backward
        dy = (y - t) / batch_num
        grads['W2'] = np.dot(z1.T, dy)
        grads['b2'] = np.sum(dy, axis=0)
        
        da1 = np.dot(dy, W2.T)
        dz1 = sigmoid_grad(a1) * da1
        grads['W1'] = np.dot(x.T, dz1)
        grads['b1'] = np.sum(dz1, axis=0)

        return grads

# 미니배치 학습 구현
import matplotlib.pyplot as plt
from dataset.mnist import load_mnist

# 데이터 읽기
(x_train, t_train), (x_test, t_test) = load_mnist(normalize=True, one_hot_label=True)

network = TwoLayerNet(input_size=784, hidden_size=50, output_size=10)

# 하이퍼파라미터
iters_num = 10000  # 반복 횟수를 적절히 설정한다.
train_size = x_train.shape[0]
batch_size = 100   # 미니배치 크기
learning_rate = 0.1

train_loss_list = []
train_acc_list = []
test_acc_list = []

# 1에폭당 반복 수
iter_per_epoch = max(train_size / batch_size, 1)

for i in range(iters_num):
    # 미니배치 획득
    batch_mask = np.random.choice(train_size, batch_size)
    x_batch = x_train[batch_mask]
    t_batch = t_train[batch_mask]
    # 기울기 계산
    #grad = network.numerical_gradient(x_batch, t_batch)
    grad = network.gradient(x_batch, t_batch)
    
    # 매개변수 갱신
    for key in ('W1', 'b1', 'W2', 'b2'):
        network.params[key] -= learning_rate * grad[key]
    
    # 학습 경과 기록
    loss = network.loss(x_batch, t_batch)
    train_loss_list.append(loss)
    
    # 1에폭당 정확도 계산
    if i % iter_per_epoch == 0:
        train_acc = network.accuracy(x_train, t_train)
        test_acc = network.accuracy(x_test, t_test)
        train_acc_list.append(train_acc)
        test_acc_list.append(test_acc)
        print("train acc, test acc | " + str(train_acc) + ", " + str(test_acc))
    
    

# 그래프 그리기
markers = {'train': 'o', 'test': 's'}
x = np.arange(len(train_acc_list))
plt.plot(x, train_acc_list, label='train acc')
plt.plot(x, test_acc_list, label='test acc', linestyle='--')
plt.xlabel("epochs")
plt.ylabel("accuracy")
plt.ylim(0, 1.0)
plt.legend(loc='lower right')
plt.show()

markers = {'train': 'o', 'test': 's'}
x = np.arange(len(train_loss_list))
plt.plot(x, train_loss_list, label='train loss')
plt.xlabel("internation")
plt.ylabel("loss")
plt.ylim(0, 9.0)
plt.legend(loc='lower right')
plt.show()
```

![image](https://user-images.githubusercontent.com/100326309/160083695-27e3b9e5-a5e0-48f8-80a5-2ca695568da0.png)

![image](https://user-images.githubusercontent.com/100326309/160083651-0405c77d-19a3-4015-adf7-9b0f7294c9e7.png)

- 그래프 해석
  - 학습 횟수(epochs)이 늘어가면서 정확도(accuarcy)는 높아지고 손실 함수(loss)의 값이 줄어든다.
  - 매개변수가 서서히 데이터에 적응하고 있음을 의미한다.
  
  

---

그림 출처: 밑바닥부터 시작하는 딥러닝
