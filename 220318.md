# 220318 🍕



## 신경망과 행렬

- 신경망의 가중치는 행렬곱의 방식으로 계산됨.

![image](https://user-images.githubusercontent.com/100326309/160082516-07dc4bab-eec5-4d9b-b58c-4a395436b524.png)

```python
X = np.array([1, 2]) # (1행 * 2열)
W = np.array([[1, 3, 5],[2, 4, 6]]) # (2 * 3)
np.dot(X, W) # array([ 5, 11, 17])  # (1 * 3)
```



## 3층 신경망 구현

1. **W1: 0층 -> 1층으로 신호 전달**

![image](https://user-images.githubusercontent.com/100326309/160082544-bfe9a405-3cdb-44d5-9d5a-73470240507b.png)

```python
import numpy as np

X = np.array([1.0, 0.5])
W1 = np.array([[0.1, 0.3, 0.5], [0.2, 0.4, 0.6]])
B1 = np.array([0.1, 0.2, 0.3])

# 가중합 A1 계산
A1 = np.dot(X, W1) + B1
print(A1) # [0.3 0.7 1.1]

# 신호 전달 시 활성화 함수(h(x))로 0~1 사이의 값으로 바꿔주는 시그모이드 함수 이용
def sigmoid(x):
    return 1/(1 + np.exp(-x))

Z1 = sigmoid(A1)
print(Z1) # [0.57444252 0.66818777 0.75026011]
```



2. **W2: 1층 -> 2층으로 신호 전달**

![image](https://user-images.githubusercontent.com/100326309/160082565-afee1a56-a23e-44b5-99ab-b5661846782f.png)

```python
W2 = np.array([[0.1,0.4],[0.2,0.5],[0.3,0.6]])
B2 = np.array([0.1,0.2])

A2 = np.dot(Z1,W2)+B2 # W1를 이용해 구한 Z1을 X자리에 넣어서 활용

Z2 = sigmoid(A2)

print(A2) # [0.51615984 1.21402696]
print(Z2) # [0.62624937 0.7710107 ]
```



3. **W3 : 2층 -> 출력층** 

![image](https://user-images.githubusercontent.com/100326309/160082600-14444485-9ccf-44c4-a47f-3ae1452aee51.png)

```python
# W3 : 2층 -> 출력층 

# 마지막에 계산된 값을 그대로 출력하는 identity function(f(x)=x)
def identity_function(x): 
    return x

W3 = np.array([[0.1,0.3],[0.2,0.4]])
B3 = np.array([0.1,0.2])

A3 = np.dot(Z2, W3) + B3 # 입력값에 Z2를 넣기

Y = identity_function(A3)
print(Y) # [0.31682708 0.69627909]
```



- 구현 정리

```python
def init_network():
    network={}
    network['W1'] = np.array([[0.1,0.3,0.5],[0.2,0.4,0.6]])
    network['b1'] = np.array([0.1,0.2,0.3])
    network['W2'] = np.array([[0.1,0.4],[0.2,0.5],[0.3,0.6]])
    network['b2'] = np.array([0.1,0.2])
    network['W3'] = np.array([[0.1,0.3],[0.2,0.4]])
    network['b3'] = np.array([0.1,0.2])
    
    return network

def forward(network , x):
    W1,W2,W3 = network['W1'],network['W2'],network['W3']
    b1,b2,b3 = network['b1'],network['b2'],network['b3']
    
    a1 = np.dot(x, W1) + b1
    z1 = sigmoid(a1)

    a2 = np.dot(z1, W2) + b2
    z2 = sigmoid(a2)
    
    a3 = np.dot(z2, W3) + b3
    y = identity_function(a3)
    
    return y

network1=init_network()
x = np.array([1.0,0.5])
y = forward(network1, x)
print(y) # [0.31682708 0.69627909]
```



## logit, sigmoid, softmax의 관계

### 1. Logit

- odds에 자연로그(밑이 e인 로그)를 취한 것
- odds
  - 성공확률(p)/실패확률(1-p)
  - 실패확률 대비 성공확률
  - 어떤 사건에서 odds > 1 이라면, 성공확률 > 실패확률
  - ex) 주사위의 눈이 3-6이 나왔을 때를 성공이라고 할 때, p = 4/6, 1-p = 2/6
    - odds = 2, 즉 성공확률이 실패확률보다 2배 높다. 
- **logit은 sigmoid는 역함수 관계**



### 2. Softmax = tanH

- sigmoid를 **다중 분류**로 일반화한 것
- sigmoid가 이진분류에서의 확률값(0 ~1)을 출력한다면, softmax는 다중분류에서의 확률값(합이 1)을 출력함. 



---

그림 출처: 밑바닥부터 시작하는 딥러닝